/home/syseo/anaconda3/envs/jericho/lib/python3.7/site-packages/torch/distributions/distribution.py:134: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead
  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)
Train Epoch: 1 [0/229 (0%)]	 Sequence LL: -4775.404297 KL: 97.098999 timestep LL : -62.093033,  timestep IWAE : -62.095284
Train Epoch: 1 [40/229 (17%)]	 Sequence LL: -3810.824463 KL: 55.235901 timestep LL : -61.471848,  timestep IWAE : -61.472561
Train Epoch: 1 [80/229 (34%)]	 Sequence LL: -3685.281006 KL: 37.640053 timestep LL : -60.663742,  timestep IWAE : -60.665970
Train Epoch: 1 [120/229 (52%)]	 Sequence LL: -3993.363770 KL: 26.302797 timestep LL : -59.821209,  timestep IWAE : -59.821808
Train Epoch: 1 [160/229 (69%)]	 Sequence LL: -4241.303711 KL: 15.394475 timestep LL : -58.708309,  timestep IWAE : -58.716293
Train Epoch: 1 [200/229 (86%)]	 Sequence LL: -3072.152100 KL: 8.897967 timestep LL : -56.106216,  timestep IWAE : -56.138027
====> Epoch: 1 Average LL per sequence: -3579.1562, Average LL per timestep: -59.3893, IWAE per timestep : -59.395234
Train Epoch: 2 [0/229 (0%)]	 Sequence LL: -4022.648926 KL: 20.836948 timestep LL : -52.307888,  timestep IWAE : -52.344620
Train Epoch: 2 [40/229 (17%)]	 Sequence LL: -2725.751465 KL: 31.729820 timestep LL : -44.029408,  timestep IWAE : -44.090210
Train Epoch: 2 [80/229 (34%)]	 Sequence LL: -1935.490723 KL: 41.138596 timestep LL : -31.884455,  timestep IWAE : -31.863338
Train Epoch: 2 [120/229 (52%)]	 Sequence LL: -1440.977783 KL: 37.137436 timestep LL : -21.652138,  timestep IWAE : -21.664024
Train Epoch: 2 [160/229 (69%)]	 Sequence LL: -1242.112549 KL: 12.534410 timestep LL : -17.353075,  timestep IWAE : -17.352884
Train Epoch: 2 [200/229 (86%)]	 Sequence LL: -765.412598 KL: 5.020581 timestep LL : -14.031937,  timestep IWAE : -14.034016
====> Epoch: 2 Average LL per sequence: -1665.9072, Average LL per timestep: -27.8959, IWAE per timestep : -27.908979
Train Epoch: 3 [0/229 (0%)]	 Sequence LL: -1128.614746 KL: 8.760009 timestep LL : -14.610187,  timestep IWAE : -14.606718
Train Epoch: 3 [40/229 (17%)]	 Sequence LL: -822.976318 KL: 7.065386 timestep LL : -13.277708,  timestep IWAE : -13.275709
Train Epoch: 3 [80/229 (34%)]	 Sequence LL: -745.775635 KL: 5.150981 timestep LL : -12.322461,  timestep IWAE : -12.325287
Train Epoch: 3 [120/229 (52%)]	 Sequence LL: -808.205444 KL: 6.598329 timestep LL : -12.219718,  timestep IWAE : -12.224436
Train Epoch: 3 [160/229 (69%)]	 Sequence LL: -840.309082 KL: 9.144590 timestep LL : -11.761292,  timestep IWAE : -11.759853
Train Epoch: 3 [200/229 (86%)]	 Sequence LL: -625.430786 KL: 7.557450 timestep LL : -11.537651,  timestep IWAE : -11.544086
====> Epoch: 3 Average LL per sequence: -751.5550, Average LL per timestep: -12.4729, IWAE per timestep : -12.473785
Train Epoch: 4 [0/229 (0%)]	 Sequence LL: -957.505371 KL: 10.837331 timestep LL : -12.578712,  timestep IWAE : -12.583730
Train Epoch: 4 [40/229 (17%)]	 Sequence LL: -727.001099 KL: 9.915888 timestep LL : -11.722040,  timestep IWAE : -11.726581
Train Epoch: 4 [80/229 (34%)]	 Sequence LL: -717.363708 KL: 7.430016 timestep LL : -11.852614,  timestep IWAE : -11.850307
Train Epoch: 4 [120/229 (52%)]	 Sequence LL: -782.217102 KL: 7.761018 timestep LL : -11.809170,  timestep IWAE : -11.805964
Train Epoch: 4 [160/229 (69%)]	 Sequence LL: -806.289795 KL: 9.802247 timestep LL : -11.271317,  timestep IWAE : -11.271103
Train Epoch: 4 [200/229 (86%)]	 Sequence LL: -610.733154 KL: 7.230778 timestep LL : -11.272357,  timestep IWAE : -11.273392
====> Epoch: 4 Average LL per sequence: -703.8438, Average LL per timestep: -11.6671, IWAE per timestep : -11.667408
Train Epoch: 5 [0/229 (0%)]	 Sequence LL: -938.541504 KL: 9.186554 timestep LL : -12.356399,  timestep IWAE : -12.353765
Train Epoch: 5 [40/229 (17%)]	 Sequence LL: -716.277466 KL: 8.094732 timestep LL : -11.532982,  timestep IWAE : -11.539904
Train Epoch: 5 [80/229 (34%)]	 Sequence LL: -710.146057 KL: 6.348633 timestep LL : -11.734378,  timestep IWAE : -11.734180
Train Epoch: 5 [120/229 (52%)]	 Sequence LL: -774.339966 KL: 6.396050 timestep LL : -11.689282,  timestep IWAE : -11.703541
Train Epoch: 5 [160/229 (69%)]	 Sequence LL: -802.135925 KL: 7.590418 timestep LL : -11.199653,  timestep IWAE : -11.196601
Train Epoch: 5 [200/229 (86%)]	 Sequence LL: -605.284851 KL: 5.821424 timestep LL : -11.159457,  timestep IWAE : -11.151146
====> Epoch: 5 Average LL per sequence: -694.2597, Average LL per timestep: -11.5008, IWAE per timestep : -11.500873
====> Valid set loss: Avg. Marginal LL = -678.3057, KL = 1.5951, Marginal LL per timestep = -11.1894, IWAE bound per timestep = -11.1897
== Best valid loss! Start Testing.. 
Traceback (most recent call last):
  File "train.py", line 237, in <module>
    if val_loss < min_loss:
  File "train.py", line 162, in test
    loss, loghat, _, _, iwae_bound = model(data, mask, num_eval_particles)
  File "/home/syseo/anaconda3/envs/jericho/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/ext2/syseo/FIVO_Gumbel/model.py", line 483, in forward
    prior_logprob_ti = prior_dist.log_prob( z_t_is.detach() ) + 1e-7
  File "/home/syseo/anaconda3/envs/jericho/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py", line 208, in log_prob
    M = _batch_mahalanobis(self._unbroadcasted_scale_tril, diff)
  File "/home/syseo/anaconda3/envs/jericho/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py", line 57, in _batch_mahalanobis
    M_swap = torch.triangular_solve(flat_x_swap, flat_L, upper=False)[0].pow(2).sum(-2)  # shape = b x c
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.65 GiB total capacity; 22.43 GiB already allocated; 16.06 MiB free; 22.55 GiB reserved in total by PyTorch)
